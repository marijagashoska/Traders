# -*- coding: utf-8 -*-
"""lstm-model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wHs-ydQNDQiTYkRWEG4aTS4apP0f5KqM
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import logging

data = pd.read_csv('issuers_data_with_headers.csv')

data.head()

TIME_STEPS = 60
BATCH_SIZE = 32
EPOCHS = 10
FEATURES = ["Transaction Price", "Last Price", "Max Price", "Min Price", "Percent Change", "Volume", "Turnover"]
TARGET = "Average Price"

def preprocess_data(data, features, target, time_steps):
    """Preprocess the data: scale, create lagged sequences, and split."""
    scaler = MinMaxScaler(feature_range=(0, 1))
    data[features + [target]] = scaler.fit_transform(data[features + [target]])

    X, y = [], []
    for i in range(time_steps, len(data)):
        X.append(data[features].iloc[i - time_steps:i].values)
        y.append(data[target].iloc[i])

    return np.array(X), np.array(y), scaler

def build_lstm_model(input_shape):
    """Build an LSTM model."""
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=input_shape),
        Dropout(0.3),
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def train_and_evaluate(model, X_train, y_train, X_val, y_val):
    """Train the model and evaluate performance."""
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        verbose=1
    )
    val_preds = model.predict(X_val)
    mse = mean_squared_error(y_val, val_preds)
    mae = mean_absolute_error(y_val, val_preds)
    return model, mse, mae

def make_future_predictions(model, last_sequence, scaler, future_steps):
    predictions = []
    input_sequence = np.expand_dims(last_sequence, axis=0)
    for _ in range(future_steps):
        pred = model.predict(input_sequence, verbose=0)
        predictions.append(pred[0, 0])

        input_sequence = np.roll(input_sequence, shift=-1, axis=1)
        input_sequence[0, -1, 0] = pred
    return np.array(predictions).reshape(future_steps, 1)
all_predictions = []

for issuer in data["Issuer Code"].unique():
    issuer_data = data[data["Issuer Code"] == issuer].copy()
    issuer_data.sort_values(by="Date", inplace=True)

    if len(issuer_data) <= TIME_STEPS:
        print(f"Not enough data for issuer {issuer}. Skipping.")
        continue

    print(f"Processing issuer: {issuer}")

    X, y, scaler = preprocess_data(issuer_data, FEATURES, TARGET, TIME_STEPS)

    split_idx = int(0.8 * len(X))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
    model, mse, mae = train_and_evaluate(model, X_train, y_train, X_val, y_val)

    print(f"Issuer {issuer} - MSE: {mse:.4f}, MAE: {mae:.4f}")

    last_sequence = X_val[-1]
    future_steps = 30
    future_predictions = make_future_predictions(model, last_sequence, scaler, future_steps)

    future_predictions_padded = np.hstack([future_predictions, np.zeros((future_predictions.shape[0], 7))])

    future_predictions = scaler.inverse_transform(future_predictions_padded)

    average_price_predictions = future_predictions[:, 0]

    all_predictions.append({
        "Issuer": issuer,
        "Future Predictions": average_price_predictions.flatten().tolist()
    })



predictions_df = pd.DataFrame(all_predictions)
predictions_df.to_csv("predicted_average_prices.csv", index=False)
print("Predictions saved to 'predicted_average_prices.csv'.")